{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read tokenized txt and conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def align_span_and_token(text_path):\n",
    "    text = None\n",
    "    with open(text_path, \"r\") as fr:\n",
    "        text = fr.read()\n",
    "#     span_and_token = []\n",
    "    tokenized_sent = [word_tokenize(sent) for sent in sent_tokenize(text)]\n",
    "    \n",
    "    span_and_token = []\n",
    "    span_start_index = 0\n",
    "    token_index = 0\n",
    "#     print(tokenized_sent)\n",
    "    for sent in tokenized_sent:\n",
    "        for token in sent:\n",
    "#             print(token)\n",
    "            find_start = False\n",
    "            while find_start == False:\n",
    "                if text[span_start_index: span_start_index+1] == token[0]:\n",
    "                    find_start = True\n",
    "                if find_start == False: \n",
    "                    span_start_index+=1\n",
    "                    continue\n",
    "                find_token = False\n",
    "                for span_end_index in range(span_start_index+1, len(text)+1):\n",
    "                    if str(text[span_start_index: span_end_index]) == str(token):\n",
    "                        span_and_token.append([span_start_index, span_end_index, token, token_index])\n",
    "                        token_index+=1\n",
    "                        span_start_index = span_end_index\n",
    "                        find_token = True\n",
    "                        break\n",
    "                if find_token == False:\n",
    "                    print(\"error. can not find the tokne.\", token, span_start_index)\n",
    "            \n",
    "            \n",
    "            \n",
    "        span_start_index+=1\n",
    "        span_and_token.append([]) # [] means the end of the sentence\n",
    "\n",
    "    \n",
    "#     print(span_and_token)\n",
    "    span_and_token_count = 0\n",
    "    for s_a_t in span_and_token:\n",
    "        if s_a_t != []:\n",
    "            span_and_token_count+=1\n",
    "    assert(span_and_token_count == sum([len(token) for token in tokenized_sent]))\n",
    "\n",
    "    return span_and_token  # [span_index_start, span_index_end, txt, token_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ann(file_name):\n",
    "    \"\"\" read the file of ann and matain entity label and relationship \"\"\"\n",
    "    mentions = {}\n",
    "    mentions_span = {}\n",
    "#     mentions_type = {}\n",
    "    relations = {}\n",
    "#     relation_type = [\"Coreference\", \"Transformed\", \"Reaction-associated\", \"Work-up\", \"Contained\"]\n",
    "    relation_type = [\"Coreference\", \"Transformed\", \"Ingredient_without-state-change_associated\",\\\n",
    "                     \"Ingredient_with-state-change_associated\"]\n",
    "    for r_type in relation_type:\n",
    "        relations[r_type] = []\n",
    "        \n",
    "    with open(file_name, \"r\") as fr:\n",
    "        lines = fr.readlines()\n",
    "        for line in lines:\n",
    "#             print(line)\n",
    "            ###filter out some unrelated label from NER\n",
    "            split_line = line.split(\"\\t\")\n",
    "            label = split_line[1].split(\" \")[0]\n",
    "\n",
    "            ## mention\n",
    "            if \"T\" in split_line[0]:\n",
    "                ## text\n",
    "                mention_text = split_line[2].split(\"\\n\")[0]\n",
    "                mentions[split_line[0]] = mention_text\n",
    "\n",
    "                ### span\n",
    "                span = split_line[1].split(\";\")\n",
    "#               discontious mention:\n",
    "                if len(span)> 1: \n",
    "                    mentions_span[split_line[0]] = []\n",
    "                    for sp in span:\n",
    "                        s = sp.split(\" \")\n",
    "                        mentions_span[split_line[0]].append((int(s[-2]), int(s[-1])))\n",
    "#                     contious mention:\n",
    "                else:\n",
    "                    span = split_line[1].split(\" \")\n",
    "                    mentions_span[split_line[0]] = [(int(span[1]), int(span[2]))]\n",
    "#           \n",
    "                ### mention_type \n",
    "#                     print(split_line)\n",
    "#                 ty = split_line[1].split(\" \")[0]\n",
    "#                 mentions_type[ann_file][split_line[0]] = ty\n",
    "\n",
    "#             R3\tTransformed Arg1:T13 Arg2:T5\t\n",
    "            ## relation    \n",
    "            if \"R\" in split_line[0]:\n",
    "                r_element = split_line[1].split(\" \")\n",
    "                r_type = r_element[0]\n",
    "                r_anaphor_t = r_element[1].split(\":\")[1]\n",
    "                r_antecedent_t = r_element[2].split(\":\")[1]\n",
    "                relations[r_type].append(list([r_anaphor_t, r_antecedent_t]))\n",
    "\n",
    "    return mentions, mentions_span, relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mentions, mentions_span, relations = process_ann(\"./annotated_data/101115.ann\")\n",
    "# mentions, mentions_span, relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_spanID_to_tokenID(lines, mentions_span):\n",
    "    mentions_token = {}\n",
    "    for t_key in mentions_span.keys():\n",
    "        spans = mentions_span[t_key]\n",
    "        # maybe discontinuous mention \n",
    "        \n",
    "        token_temp = []\n",
    "        for s in spans:\n",
    "#             print(s)\n",
    "            start_t = None\n",
    "            end_t = None\n",
    "            \n",
    "            start_s = s[0]\n",
    "            end_s = s[1]\n",
    "            \n",
    "            find_start_id = False\n",
    "            find_end_id = False\n",
    "            \n",
    "            for line in lines: \n",
    "                if line == []:\n",
    "                    continue\n",
    "                if line[0] == start_s:\n",
    "                    find_start_id = True\n",
    "                    start_t = line[3]\n",
    "                if line[1] == end_s: \n",
    "                    find_end_id = True\n",
    "                    end_t = line[3]\n",
    "                \n",
    "                if find_start_id and find_end_id: \n",
    "                    break\n",
    "            \n",
    "            \n",
    "            if find_start_id and not find_end_id:\n",
    "                print(\"A: something wrong with the ID matching - only find the start of span\")\n",
    "                print(\"spans:\", spans)\n",
    "                print(\"span_token: \", lines)\n",
    "                end_t = start_t\n",
    "                print(\"then get token: \", [start_t, end_t])\n",
    "                \n",
    "            if not find_start_id and find_end_id:\n",
    "                print(\"A: something wrong with the ID matching - only find the end of span\")\n",
    "                print(\"spans:\", spans)\n",
    "                print(\"span_token: \", lines)\n",
    "                start_t = end_t                \n",
    "                print(\"then get token: \", [start_t, end_t])\n",
    "                \n",
    "            if not find_start_id and not find_end_id: # get the brodar span:\n",
    "                for line_index in range(len(lines)-1):\n",
    "                    \n",
    "                    next_index = line_index +1 \n",
    "                    if lines[line_index] == []:\n",
    "                        continue\n",
    "                    if lines[line_index+1] == []:\n",
    "                        next_index = line_index\n",
    "\n",
    "                    if lines[line_index][0] <= start_s and lines[next_index][0] >= start_s:\n",
    "                        find_start_id = True\n",
    "                        start_t = lines[line_index][3]\n",
    "                        \n",
    "                    if lines[line_index][1] <= end_s and lines[next_index][1] >= end_s: \n",
    "                        find_end_id = True\n",
    "                        end_t = lines[next_index][3]\n",
    "\n",
    "                    if find_start_id and find_end_id: \n",
    "                        break\n",
    "                    \n",
    "            if not find_start_id or not find_end_id: # still nothing so we discard them      \n",
    "                continue\n",
    "                \n",
    "            if start_t > end_t:\n",
    "                print(\"b: something wrong with the ID matching - token: start is larger than end\")\n",
    "#             print([start_t, end_t])   \n",
    "            token_temp.append([start_t, end_t])   \n",
    "        mentions_token[t_key]= token_temp\n",
    "\n",
    "    return mentions_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def constcut_jsonline_format(doc_id, lines, mentions_token, relations):\n",
    "    # get sentences \n",
    "    instance = {}\n",
    "#     Index(['speakers', 'doc_key', 'sentences', 'constituents', 'clusters', 'ner'], dtype='object')\n",
    "    instance[\"doc_key\"] = doc_id\n",
    "    \n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in lines:\n",
    "        if line == []:\n",
    "            sentences.append(sentence)\n",
    "            sentence = []\n",
    "            continue\n",
    "        else:\n",
    "            sentence.append(line[2])\n",
    "#     relations_types = relations.keys() # coref, TR, RA, WU, CA\n",
    "\n",
    "    instance[\"sentences\"] = sentences\n",
    "    \n",
    "    #useless keys\n",
    "    # speaker\n",
    "#     instance[\"speakers\"] = []\n",
    "#     for s in sentences:\n",
    "#         instance[\"speakers\"].append([\"-\"]*len(s))\n",
    "#     instance[\"constituents\"] = []\n",
    "#     instance[\"ner\"] = []\n",
    "    \n",
    "    # relation keys\n",
    "    relations_token = {}\n",
    "    for r_type in relations:\n",
    "        relations_token[r_type] = []\n",
    "        for anphor_antecedent_pair in relations[r_type]:\n",
    "            relations_token[r_type].append([mentions_token[anphor_antecedent_pair[0]],mentions_token[anphor_antecedent_pair[1]]])\n",
    "        \n",
    "        instance[r_type] = relations_token[r_type]\n",
    "\n",
    "    return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "revise = True\n",
    "\n",
    "root_path = \"/home/biaoyan/data/Recipe/RecipeRef/\"\n",
    "\n",
    "data_type = \"train\"\n",
    "\n",
    "if not revise:\n",
    "\n",
    "    annotation_path = \"%s/%s/\"%(root_path, data_type)\n",
    "else: \n",
    "    annotation_path = \"%s/%s_no_state/\"%(root_path, data_type)\n",
    "\n",
    "\n",
    "    \n",
    "annotation_file_name = os.listdir(annotation_path)\n",
    "annotation_ann_name = []\n",
    "annotation_txt_name = []\n",
    "annotation_doc_id = []\n",
    "\n",
    "for file in annotation_file_name:\n",
    "    if \".ann\" in file:\n",
    "        annotation_ann_name.append(file)\n",
    "        annotation_doc_id.append(file.split(\".\")[0])        \n",
    "    elif \".txt\" in file:\n",
    "        annotation_txt_name.append(file)\n",
    "annotation_doc_id = sorted(annotation_doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(annotation_doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "for doc_id in annotation_doc_id:\n",
    "    print(doc_id)\n",
    "    if not revise: \n",
    "        jsonlines_path_part = \"%s/%s_converted_jsonlines/\"%(root_path, data_type)\n",
    "    else:\n",
    "        jsonlines_path_part = \"%s/%s_converted_jsonlines_no_state/\"%(root_path, data_type)\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(jsonlines_path_part):\n",
    "        os.makedirs(jsonlines_path_part)\n",
    "        \n",
    "    jsonlines_path = jsonlines_path_part+doc_id+\".jsonlines\" \n",
    "    \n",
    "        \n",
    "    with open(jsonlines_path, 'w') as fw:\n",
    "        txt_path = annotation_path+doc_id+\".txt\"\n",
    "#         tokenized_conll_path = \"./chemical-patents/final_anaphora_data/snippets_only_token/\"+doc_id+\".conll\"\n",
    "#         lines = align_span_and_token_from_tokenized_conll(txt_path, tokenized_conll_path)\n",
    "        lines = align_span_and_token(txt_path)\n",
    "        ann_path = annotation_path+doc_id+\".ann\"\n",
    "        mentions, mentions_span, relations = process_ann(ann_path)\n",
    "        mentions_token = transfer_spanID_to_tokenID(lines, mentions_span)\n",
    "        instance = constcut_jsonline_format(doc_id, lines, mentions_token, relations)\n",
    "\n",
    "    # acttually, we should dump this jonsonlines \n",
    "        json.dump(instance, fw)\n",
    "        fw.write(\"\\n\")\n",
    "#     instances.append(instance)\n",
    "# annotation_doc_id = sorted(annotation_doc_id)\n",
    "print(\"Done!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "def merge_jsonlines(jsonlines_path, merge_jsonlines_path):\n",
    "\n",
    "    annotation_doc_id = os.listdir(merge_jsonlines_path)\n",
    "\n",
    "    with open(jsonlines_path, 'w') as fw:\n",
    "        for doc_path in annotation_doc_id:\n",
    "            jsonlines_path_part = merge_jsonlines_path + doc_path  \n",
    "            with open(jsonlines_path_part, \"r\") as fr:\n",
    "                fw.write(fr.read())\n",
    "                \n",
    "if not revise: \n",
    "    jsonlines_path = \"%s/%s.english.jsonlines\"%(root_path, data_type)\n",
    "else:\n",
    "    jsonlines_path = \"%s/%s_no_state.english.jsonlines\"%(root_path, data_type)\n",
    "    \n",
    "# jsonlines_path = \n",
    "merge_jsonlines_path = jsonlines_path_part\n",
    "\n",
    "\n",
    "merge_jsonlines(jsonlines_path, merge_jsonlines_path)\n",
    "# get_partition(data_type, annotation_path, annotation_doc_id, start_t, end_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
